{"cells":[{"cell_type":"markdown","metadata":{"id":"EB9H7BS_UH1E"},"source":["# Aspect-Based Sentiment Classification Walkthrough\n","\n","In this notebook, we will explore the process of using LangChain with a large language model (LLM) for aspect-based sentiment classification (ABSC). ABSC is a granular approach to sentiment analysis that determines the sentiment (positive, negative, or neutral) toward specific aspects or attributes within a piece of text.\n","\n","For example, in the sentence:\n","> \"The food at the restaurant was delicious, but the service was slow.\"\n","\n","The sentiment toward the aspect *\"food\"* is positive, while the sentiment toward the aspect *\"service\"* is negative.\n","\n","### Outline\n","\n","In this walkthrough, we will:\n","\n","1. **Load an ABSC Dataset:** Read in a dataset specifically designed for aspect-based sentiment classification. We will use the SemEval 2014 Dataset, which can be downloaded in  handy csv file from [Kaggle](https://www.kaggle.com/datasets/charitarth/semeval-2014-task-4-aspectbasedsentimentanalysis?select=Laptop_Train_v2.csv)\n","2. **Build an LLM Using LangChain and HuggingFace:** Configure the large language model to handle sentiment classification tasks.\n","3. **Craft a Labeling Prompt:** Create a well-structured prompt to guide the LLM in identifying sentiment for specific aspects of text.\n","4. **Classify Dataset Examples:** Use the LLM and the prompt to classify examples in the dataset.\n","5. **Evaluate Performance:** Measure classification accuracy using evaluation metrics such as precision, recall, and F1 score.\n","\n","### Example\n","\n","To understand ABSC better, let’s consider this example:\n","\n","```python\n","review = \"The laptop's performance is outstanding, but the battery life is disappointing.\"\n","aspects = [\"performance\", \"battery life\"]\n","```\n","\n","The expected output is:\n","\n","| Aspect         | Sentiment   |\n","|----------------|-------------|\n","| Performance    | Positive    |\n","| Battery Life   | Negative    |\n","\n","By the end of this notebook, you'll learn how to apply ABSC with LLMs to solve similar problems effectively.\n"]},{"cell_type":"markdown","metadata":{"id":"PCM6JGuoUH1I"},"source":["# Configure the Environment"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6WOciQ0TUH1J","executionInfo":{"status":"ok","timestamp":1739975846893,"user_tz":-330,"elapsed":102171,"user":{"displayName":"Manu Gupta","userId":"06872556596276847349"}},"outputId":"c61483ec-634b-426b-96e5-69d845815265"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.18)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.35)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n","Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain) (3.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Collecting langchain-huggingface\n","  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.28.1)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.3.35)\n","Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (3.4.1)\n","Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.0)\n","Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (4.48.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.17.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.3.8)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (9.0.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n","Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.10.6)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.5.1+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.1.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.11.6)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.2)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.15)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2025.1.31)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.5)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.14.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.3.1)\n","Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, langchain-huggingface\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed langchain-huggingface-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}],"source":["! pip install pandas\n","! pip install langchain\n","! pip install transformers\n","! pip install langchain-huggingface"]},{"cell_type":"markdown","metadata":{"id":"LXakdEC1UH1K"},"source":["# Read in dataset and investigate the data\n","\n","Make sure to download the data from [Kaggle](https://www.kaggle.com/datasets/charitarth/semeval-2014-task-4-aspectbasedsentimentanalysis?select=Laptop_Train_v2.csv). We'll be working with the ```Laptop_Train_v2.csv```\n","\n","__Prompt__: I would like the python code to read in python dataframe from a csv named \"Laptop_Train_v2.csv\". I would like for you to downsample to 100 entries from the dataframe. Note that for the sampling, use the column 'id' to determine samples. So, there should be 100 unique values of 'id', but there will be more than 100 rows. I would like to also have the code to view the column names, a sample of the entries and summary of the values in each column."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggfNSE7wUH1K","executionInfo":{"status":"ok","timestamp":1739975847615,"user_tz":-330,"elapsed":723,"user":{"displayName":"Manu Gupta","userId":"06872556596276847349"}},"outputId":"d131e281-7187-449e-d5a9-8b0efc064401"},"outputs":[{"output_type":"stream","name":"stdout","text":["CSV file successfully loaded!\n"]}],"source":["import pandas as pd\n","\n","file_name = \"Laptop_Train_v2.csv\"\n","df = pd.read_csv(file_name)\n","print(\"CSV file successfully loaded!\")\n","\n","# Downsample to 100 unique 'id' values\n","sampled_ids = df['id'].drop_duplicates().sample(n=100)  # Randomly select 100 unique IDs\n","df = df[df['id'].isin(sampled_ids)]"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qATnM8ZPUH1L","executionInfo":{"status":"ok","timestamp":1739975847639,"user_tz":-330,"elapsed":4,"user":{"displayName":"Manu Gupta","userId":"06872556596276847349"}},"outputId":"5e983eb9-9acd-4aa7-8b6a-ef47bd93acbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Column Names:\n","Index(['id', 'Sentence', 'Aspect Term', 'polarity', 'from', 'to'], dtype='object')\n"]}],"source":["# Display the column names\n","print(\"\\nColumn Names:\")\n","print(df.columns)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9x_4HdKxUH1L","executionInfo":{"status":"ok","timestamp":1739975847740,"user_tz":-330,"elapsed":87,"user":{"displayName":"Manu Gupta","userId":"06872556596276847349"}},"outputId":"9dc04c97-5cc6-486d-dd5c-c3c5bd76bd97"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sample Entries:\n","     id                                           Sentence       Aspect Term  \\\n","45  810  YOU WILL NOT BE ABLE TO TALK TO AN AMERICAN WA...  WARRANTY SERVICE   \n","56  997  Drivers updated ok but the BIOS update froze t...           Drivers   \n","57  997  Drivers updated ok but the BIOS update froze t...       BIOS update   \n","58  997  Drivers updated ok but the BIOS update froze t...            system   \n","61  147                         The keyboard is too slick.          keyboard   \n","\n","    polarity  from  to  \n","45  negative    44  60  \n","56  positive     0   7  \n","57  negative    27  38  \n","58  negative    49  55  \n","61  negative     4  12  \n"]}],"source":["# Display a sample of the entries (first 5 rows by default)\n","print(\"\\nSample Entries:\")\n","print(df.head())\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8V3l1PG8UH1L","executionInfo":{"status":"ok","timestamp":1739975847747,"user_tz":-330,"elapsed":21,"user":{"displayName":"Manu Gupta","userId":"06872556596276847349"}},"outputId":"7407e481-0b86-486a-e2e1-b4db6ece6455"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Summary of Column Values:\n","<class 'pandas.core.frame.DataFrame'>\n","Index: 143 entries, 45 to 2319\n","Data columns (total 6 columns):\n"," #   Column       Non-Null Count  Dtype \n","---  ------       --------------  ----- \n"," 0   id           143 non-null    int64 \n"," 1   Sentence     143 non-null    object\n"," 2   Aspect Term  143 non-null    object\n"," 3   polarity     143 non-null    object\n"," 4   from         143 non-null    int64 \n"," 5   to           143 non-null    int64 \n","dtypes: int64(3), object(3)\n","memory usage: 7.8+ KB\n","None\n"]}],"source":["# Display a summary of the values in each column\n","print(\"\\nSummary of Column Values:\")\n","print(df.info())  # Provides information about data types and non-null counts"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"upcqdC7OUH1M","executionInfo":{"status":"ok","timestamp":1739975847747,"user_tz":-330,"elapsed":18,"user":{"displayName":"Manu Gupta","userId":"06872556596276847349"}},"outputId":"36a7033f-a4ac-4d51-bb8f-b9a5e02eec20"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Statistical Summary:\n","                 id                                           Sentence  \\\n","count    143.000000                                                143   \n","unique          NaN                                                100   \n","top             NaN  But to be honest, I don't use my computer for ...   \n","freq            NaN                                                  3   \n","mean    1742.874126                                                NaN   \n","std      900.760140                                                NaN   \n","min        3.000000                                                NaN   \n","25%     1056.000000                                                NaN   \n","50%     1890.000000                                                NaN   \n","75%     2506.000000                                                NaN   \n","max     3077.000000                                                NaN   \n","\n","       Aspect Term  polarity        from          to  \n","count          143       143  143.000000  143.000000  \n","unique         109         3         NaN         NaN  \n","top       keyboard  positive         NaN         NaN  \n","freq             8        64         NaN         NaN  \n","mean           NaN       NaN   50.587413   59.923077  \n","std            NaN       NaN   49.966937   49.675720  \n","min            NaN       NaN    0.000000    4.000000  \n","25%            NaN       NaN   16.000000   26.000000  \n","50%            NaN       NaN   36.000000   47.000000  \n","75%            NaN       NaN   68.000000   77.500000  \n","max            NaN       NaN  301.000000  306.000000  \n"]}],"source":["print(\"\\nStatistical Summary:\")\n","print(df.describe(include='all'))  # Provides a statistical summary for all columns"]},{"cell_type":"markdown","metadata":{"id":"bg9Rozk0UH1M"},"source":["__prompt__: Okay, now please give me the python code to look at the unique values, along with their counts of the \"polarity\" and the \"Aspect Term\" columns from the dataframe 'df'. You do not need to check for the columns or dataframe; they are already loaded in."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfCVx0AmUH1M","executionInfo":{"status":"ok","timestamp":1739975847751,"user_tz":-330,"elapsed":18,"user":{"displayName":"Manu Gupta","userId":"06872556596276847349"}},"outputId":"4994e75f-db1d-49a8-8176-71d07371175a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Unique values and their counts in the 'polarity' column:\n","polarity\n","positive    64\n","negative    55\n","neutral     24\n","Name: count, dtype: int64\n"]}],"source":["# Display unique values and their counts for \"polarity\"\n","print(\"\\nUnique values and their counts in the 'polarity' column:\")\n","print(df['polarity'].value_counts())"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsizZCalUH1M","executionInfo":{"status":"ok","timestamp":1739975847856,"user_tz":-330,"elapsed":104,"user":{"displayName":"Manu Gupta","userId":"06872556596276847349"}},"outputId":"81961b42-cd45-4e43-838d-414797d400c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Unique values and their counts in the 'Aspect Term' column:\n","Aspect Term\n","keyboard                    8\n","screen                      6\n","programs                    6\n","price                       4\n","size                        4\n","                           ..\n","windows disc                1\n","Programs                    1\n","value                       1\n","Windows operating system    1\n","compatibility               1\n","Name: count, Length: 109, dtype: int64\n"]}],"source":["# Display unique values and their counts for \"Aspect Term\"\n","print(\"\\nUnique values and their counts in the 'Aspect Term' column:\")\n","print(df['Aspect Term'].value_counts())"]},{"cell_type":"markdown","metadata":{"id":"eEpbOaNYUH1M"},"source":["# Instantiate a LLM for Classification\n","\n","__prompt__: Now, I need you to create an LLM object using LangChain. In particular, I would like to use the text-generation model of \"tiiuae/Falcon3-3B-Instruct\" from HuggingFace and use the 0th GPU. Make sure to import the langchain HuggingFace pipeline as \"from langchain_huggingface import HuggingFacePipeline\". Also, make sure when creating the pipeline to specify \"max_new_tokens = 500\", and make sure the pipeline only outputs the generated text and not the prompt."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338,"referenced_widgets":["4cb236b211a84a57a9df9dc2b59ca915","88a9ed5c779c44e39df501f8fab9984e","fe57e5954cb9493f84b4842f810ec7b0","280408967ec646ba9c9d765face6209d","0b3713c2afd74a9fb23dfe153e33d9d0","03a40de991a24cb5810ba52c82965df2","6fabeba2bfe442e794cd32426cd0e70e","c44de7799a4147798e608b587b6b966d","d900cc81bc69430c8fad97c840cd29a8","60ed0dd5a20a435792dcb26f759dd064","9cd05ffce7ba48ed9713b8a7d0c66edf","fa0c6fe5371e4b84b76ff74d8dcc0b3d","066ad96fe8e04757bfaf439aa08b99dd","7f498a16eb764401977885e20d88349d","95b30256ab61473fafb41ae7b754bd1e","5a0f25d243704573a23889f813f73908","bb7356f4f28d4cc3acfef33112dbf604","b88eddfcfc0249e5a02876e9db25dba3","a746ae7230b245018c4433b3d509a896","a2ecb84dc40c4962a9c2b88e25e6f30b","841606ef0aa54d7a9ee8f1e0e6506ac0","d6ecfcebe3df4689b44f4f28ec52b221","fc7176fcc2e7413e84f8d9ce94273ddd","7bfc3981288b451aa339fb6386d62c34","a1ecc6d4cba34a89840a1aacdb8dddd8","c8f4c01561ba4e47a35ef78348ab90b1","45a3a00f6ca946c88b213f14bfab60b2","592e69d898734d74ab1dbcccc7a9a11f","622984569a3548629fd40740244bba2e","6a797df0a0844e8cb6d04358a1ee17ba","8ea03289b0c4482ba118d8b90b191643","5b8d1f66063e468f8b7564a3e43dc9a7","5290097e466f4d0fbfbd1ba8a697ed6d","a5defcb1bad746208822be2d5eaca551","904485f2c6be445b976642dedfa17191","119d9d5686fa402f8c9b302a3d986d28","74bab200d75c4b0d9c6b9225bb1e0d29","d6e524297c794f019adfc97c2f6dcb89","dd0e396baad74b32aee4e05c8de4ad32","7cdcd3800bbb4d72b13eaf0f828b8d2e","ca173a48b18a4126b767252f0a2aecb0","be64c4a5424849ef8b2af663db39a45e","e28434a1de6a48ddaeccaf1590748406","36601d9fb7394e3e893114d195ad4730","fa9c171a2310406eb5256d9fcf3f45a1","e1f0f3ab916e4c099b9e143ab4739ed7","5dfba9dbb0fa4edcb49e21d44614e77e","df6cb4a6a9654a58ab2ca53975f19cb7","797d0b7040d54cf790793ea1f4a3cfbf","03af709f35044e69b3c3b1bf25e1e21c","3a58ba2889fc44d5940cd44d94af26ee","738228c6c1fb45a1a3cb8f43d9ecbcef","01bf8028ff9248efad7933fb51b3baf3","4045ba6f7fda488ab54b378a41a23f5e","ef612e194fc54dd99fc5a465b25146bf","6c7fce0ca53d44b9902cf6a5abfe7581","9f689d078f13482280c1feb7822a1a12","44a38777069d4e9e8d82ee8611d406b0","71952ec84dbe46e38eaa446d98d74f6e","e2cc8ca3212e492b8e890d4c22b41066","c98d1c4cb71a494697cdf9c5565c00a4","617260252b0b4dd09a36a5496ac59dfe","2e5d81217ae74e39917ec20873a91648","c96fa41a542e488da9ee8b9e3bed1476","9c9c152afec34198a3f8dc57ad38e8f3","1fc004033cbe41fb88e89cbb929043fc"]},"id":"gK78ZmJXUH1M","outputId":"274795f0-ee8a-4818-eb25-651194319025"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb236b211a84a57a9df9dc2b59ca915"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa0c6fe5371e4b84b76ff74d8dcc0b3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc7176fcc2e7413e84f8d9ce94273ddd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5defcb1bad746208822be2d5eaca551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.47G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa9c171a2310406eb5256d9fcf3f45a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c7fce0ca53d44b9902cf6a5abfe7581"}},"metadata":{}}],"source":["# Import the required libraries\n","from langchain_huggingface import HuggingFacePipeline\n","from transformers import pipeline\n","\n","# Define the model name\n","model_name = \"tiiuae/Falcon3-3B-Instruct\"\n","\n","# Create a HuggingFace pipeline with the specified settings\n","text_gen_pipeline = pipeline(\n","    \"text-generation\",  # Specify the task\n","    model=model_name,   # Specify the model\n","    device=0,           # Use the 0th GPU\n","    max_new_tokens=500, # Limit the number of generated tokens\n","    return_full_text=False  # Ensure the output only includes the generated text\n",")\n","\n","# Wrap the pipeline with LangChain's HuggingFacePipeline\n","llm = HuggingFacePipeline(pipeline=text_gen_pipeline)"]},{"cell_type":"code","source":["llm(\"Say Hello\")"],"metadata":{"id":"cUkUK_dVYfJ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bGsejLRsUH1N"},"source":["# Label the Aspects and Sentiments\n","\n","Now, we need to label the aspects and sentiments in the data. We first need to create an aspect sentiment labeling prompt\n","\n","__prompt__: I would like a prompt, formatted as a langchain prompt object, that does aspect-based sentiment classification of a laptop review. The prompt takes in a review of a laptop called {sentence} and does two things. First, it needs to determine what aspects are mentioned in the review. Examples include: 'screen', 'battery life', 'packaging', 'graphics', 'warranty', 'price', 'features'. Each review should have between 1 and 4 aspects, and aspects are usually only one or two word phrases from the review text that describe the laptop. Aspects are usually nouns, and not adverbs or adjectives like \"died\" or \"perfect\".\n","\n","Then for each aspect that is in the review - and only those aspects - it needs to provide the sentiment of the review towards that aspect. The possible sentiment values are \"positive\", \"negative\", \"neutral\", and \"conflict\". The prompt should also specify that the output should be of the form of a list of tuples, where the first entry in the tuple is the aspect, and second entry is the sentiment of the review towards that aspect. The prompt should also specify that the LLM should only output this list of tuples and no other words. So, as an example:\n","\n","sentence: \"The Macbook arrived in a nice twin packing and sealed in the box, all the functions works great.\"\n","output: [('functions', 'positive'),('packaging', 'positive')]\n","\n","sentence: \"The USB port never worked\"\n","output: [('USB port', 'negative')]\n","\n","sentence: \"The price and features more than met my needs.\"\n","output: [('price', 'positive'), ('features', 'positive')]\n","\n","sentence: \"My warranty ran out right as the screen died.\"\n","output: [('warranty', 'negative'), ('screen', 'negative')]\n","\n","sentence: \"The battery has standard life and the shipping was fast.\"\n","output: [('battery', 'neutral'), ('shipping', 'positive')]\n","\n","sentence: \"Just a black screen!\"\n","output: [('screen', 'negative')]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bniFGVJkUH1N"},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","\n","# Define the template for aspect-based sentiment classification\n","template = \"\"\"\n","You are an AI that analyzes laptop reviews and determines the sentiment towards various aspects mentioned in the review.\n","\n","Given a review sentence, do the following:\n","1. Identify the aspects mentioned in the review. Aspects are typically nouns, usually one or two words, and represent features of the laptop (such as 'screen', 'battery life', 'packaging', 'graphics', 'warranty', 'price', 'features'). There should be between 1 and 4 aspects mentioned in the review.\n","2. For each identified aspect, determine the sentiment expressed in the review towards that aspect. The possible sentiments are 'positive', 'negative', 'neutral', or 'conflict'.\n","3. Provide the output as a list of tuples, where the first entry in the tuple is the aspect, and the second entry is the sentiment towards that aspect.\n","\n","The output should only be the list of tuples with the aspects and sentiments and nothing else. Below are some examples:\n","\n","Example 1:\n","Input: \"The Macbook arrived in a nice twin packing and sealed in the box, all the functions works great.\"\n","Output: [('functions', 'positive'), ('packaging', 'positive')]\n","\n","Example 2:\n","Input: \"The USB port never worked.\"\n","Output: [('USB port', 'negative')]\n","\n","Example 3:\n","Input: \"The price and features more than met my needs.\"\n","Output: [('price', 'positive'), ('features', 'positive')]\n","\n","Example 4:\n","Input: \"My warranty ran out right as the screen died.\"\n","Output: [('warranty', 'negative'), ('screen', 'negative')]\n","\n","Example 5:\n","Input: \"The battery has standard life and the shipping was fast.\"\n","Output: [('battery', 'neutral'), ('shipping', 'positive')]\n","\n","Example 6:\n","Input: \"Just a black screen!\"\n","Output: [('screen', 'negative')]\n","\n","Input: \"{sentence}\"\n","Output:\n","\"\"\"\n","\n","# Create the LangChain prompt object\n","prompt = PromptTemplate(input_variables=[\"sentence\"], template=template)\n","\n","# Example usage\n","sentence = \"The battery life was long, but the screen quality was disappointing.\"\n","formatted_prompt = prompt.format(sentence=sentence)\n","print(formatted_prompt)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3oNgaVhNUH1N"},"outputs":[],"source":["example = df.iloc[10,:]\n","\n","print(example['Sentence'])\n","\n","llm(prompt.format(sentence=example['Sentence']))"]},{"cell_type":"markdown","metadata":{"id":"iIoPXuIyUH1N"},"source":["Now that we have our labeling prompt, lets construct our labeling chain.\n","\n","__prompt__: Now, given the LangChain prompt template \"prompt\", please give me the code to create a langchain chain using the pipe operator \"|\" with the prompt and an LLM called \"llm\". Please also create function to parse the output and remove extraneous output from the llm. Below are some examples:\n","\n","'\\n<|assistant|>\\n[output] \"[\\'hardware\\', \\'positive\\'], [\\'shipping\\', \\'negative\\']\"' -> [('hardware', 'positive'), ('shipping', 'negative')]\n","\"<|assistant|>\\n['camera', 'neutral']\" -> [('camera', 'neutral')]\n","\"<|assistant|>\\n[['graphics', 'positive']], \" -> [('graphics', 'positive')]\n","\"<|assistant|>\\n[('touchpad', 'positive')]\" -> [('touchpad', 'positive')]\n","\"<|assistant|>\\nsomething nonsensical, \" -> [('','')]\n","\n","The chain should resemble: label_prompt | llm | parse_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDxDFX64UH1N"},"outputs":[],"source":["import ast\n","\n","# Function to parse the LLM output and remove extraneous content\n","def parse_output(output: str):\n","    try:\n","        # Extract the list of tuples from the LLM output, which could be wrapped in extra characters\n","        parsed_output = ast.literal_eval(output.strip().replace('<|assistant|>', '').strip())\n","\n","        # Ensure the output is a list of tuples\n","        if isinstance(parsed_output, list) and all(isinstance(item, tuple) and len(item) == 2 for item in parsed_output):\n","            return parsed_output\n","        else:\n","            return [('','')]  # Return an empty tuple if the output isn't valid\n","    except Exception as e:\n","        # Handle errors and return empty tuple in case of invalid format\n","        print(f\"Error parsing output: {e}\")\n","        return [('','')]\n","\n","# Define the LLM chain that uses the prompt and LLM\n","chain = prompt | llm | parse_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DCPMyRtfUH1N"},"outputs":[],"source":["print(example['Sentence'])\n","\n","chain.invoke(example['Sentence'])"]},{"cell_type":"markdown","metadata":{"id":"5DUUxdiNUH1N"},"source":["__prompt__: Now, produce code to iterate through the dataframe \"df\" and do the aspect-based sentiment classifications of the \"Sentence\" column of the dataframe. Please note that you only need to do a labeling for each unique entry in 'id' column. Store the output as a new dataframe called \"results_df\" with columns 'id' and \"aspect_sentiment\". Please also include tqdm to monitor performance into the labeling loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ytA4HViwUH1N"},"outputs":[],"source":["from tqdm import tqdm\n","results = []\n","\n","# Iterate through each unique 'id' in the DataFrame using tqdm to monitor progress\n","for unique_id in tqdm(df['id'].drop_duplicates(), desc=\"Processing reviews\", unit=\"id\"):\n","    # Get the first sentence for each unique 'id'\n","    sentence = df[df['id'] == unique_id].iloc[0]['Sentence']\n","\n","    # Get the aspect-based sentiment output\n","    aspect_sentiment = chain.invoke(sentence)\n","\n","    # Append the result to the results list\n","    results.append({'id': unique_id, 'aspect_sentiment': aspect_sentiment})\n","\n","# Create the results DataFrame\n","results_df = pd.DataFrame(results)\n","\n","# Show the first few rows of the results DataFrame\n","print(results_df.head())"]},{"cell_type":"markdown","metadata":{"id":"PHFLEVk1UH1O"},"source":["# Evaluate the Results\n","\n","Now, we need to evaluate the results. One of the difficulties, however, is in the phrasing of the aspects. Its possible to have more than one term describe the same aspect, such as \"battery\" and \"battery life\" in \"the battery life is really good\". In both cases, we are talking about the \"battery\" but an exact match of the aspect words would fail. So, we will use the LLM to assist us in the evaluation of results by comparing entities and mathcing semantically close ones.\n","\n","First, we need to get out all of the aspect based sentiments from the original dataframe\n","\n","__prompt__ I would like the python code to get all of the aspects and their corresponding polarities from the dataframe 'df'. To do this, for each unique id in the column 'id' take out all of the entries from the 'Aspect Term' and 'polarity' columns and combine those into a tuple. Then combine all of the tuples for each unique id into a list. Then create a dataframe with columns 'id' and 'true_aspect_sentiment' from these values. So, for example, if the id '1111' has two entries, than there should be an entry in the new dataframe of {'id':1111, 'true_aspect_sentiment':[('aspect_1', 'sentiment_1'), ('aspect_2', 'sentiment_2')]}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzPoY6kDUH1O"},"outputs":[],"source":["true_aspect_sentiment = []\n","\n","# Iterate through each unique 'id' in the DataFrame\n","for unique_id in df['id'].drop_duplicates():\n","    # Get the rows for the current unique 'id'\n","    subset = df[df['id'] == unique_id]\n","\n","    # Create a list of tuples from the 'Aspect Term' and 'polarity' columns\n","    aspect_sentiment_tuples = list(zip(subset['Aspect Term'], subset['polarity']))\n","\n","    # Append the result to the list, ensuring each entry has 'id' and 'true_aspect_sentiment'\n","    true_aspect_sentiment.append({'id': unique_id, 'true_aspect_sentiment': aspect_sentiment_tuples})\n","\n","# Create the results DataFrame\n","true_aspect_sentiment_df = pd.DataFrame(true_aspect_sentiment)\n","\n","# Show the first few rows of the results DataFrame\n","print(true_aspect_sentiment_df.head())"]},{"cell_type":"markdown","metadata":{"id":"wlj138XhUH1O"},"source":["__prompt__ now, please give me the code to join 'true_aspect_sentiment_df' with 'results_df' on the 'id' column. Call this dataframe 'eval_df'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40Gn0WNvUH1O"},"outputs":[],"source":["# Join 'true_aspect_sentiment_df' with 'results_df' on the 'id' column\n","eval_df = pd.merge(true_aspect_sentiment_df, results_df, on='id', how='inner')\n","\n","# Show the first few rows of the resulting 'eval_df'\n","print(eval_df.head())"]},{"cell_type":"markdown","metadata":{"id":"kv07huiZUH1O"},"source":["Now, let's do the matching of the results, so that we can evaluate how we our LLM and prompt are doing.\n","\n","__prompt__ Please produce prompt to match tuples between two different lists in a langchain prompt template. The LLM will be given two lists of tuples, one called \"true aspect sentiments\" and one called \"predicted aspect sentiments\". The task is to determine for each tuple in the \"true aspect sentiments\" if there is a tuple in \"predicted aspect sentiments\" that closely matches. To do the matching, there are two steps. First, you need to determine if the first entry in the tuples are describing the same things. For example, \"battery\" and \"battery life\" are roughly describing the same things, while \"operating system\" and \"packaging\" are not. In other words, you must determine if the two first entries are semantically very close. Then, if the first entries of the tuples match, compare the second entries of the tuples for matching. The second entries are the sentiment terms and should match exactly. For example \"positive\" and \"positive\" match, but \"neutral\" and \"negative\" do not. Once you have match for a tuple, move to the next tuple; for each tuple in \"true aspect sentiments\", only count if it has at least one match. Finally, output the number of matches you have as a number (i.e., 0,1,2, etc.). Below are some examples to help you format this prompt for the LLM:\n","\n","Example 1:\n","true aspect sentiments: [(suite of software, positive)]\n","predicted aspect sentiments: [(software, positive), (suite, positive)]\n","output: 1\n","\n","Example 2\n","true aspect sentiments: [('space', 'positive'), ('keyboard', 'negative')]\n","predicted aspect sentiments: [('extra space', 'positive'), ('keyboard', 'negative')]\n","output: 2\n","\n","Example 3\n","true aspect sentiments: [('price premium', 'negative'), ('features', 'positive')]\n","predicted aspect sentiments: [('price', 'neutral'), ('features', 'positive')]\n","output: 1\n","\n","Example 4\n","true aspect sentiments: [('web cam', 'neutral'), (\"burn cd's\", 'neutral')]\n","predicted aspect sentiments: [('web cam', 'negative'), ('cd burning', 'negative')]\n","output: 0\n","\n","Example 5\n","true aspect sentiments: [('space', 'positive'), ('keyboard', 'negative')]\n","predicted aspect sentiments: [('storage', 'positive'), ('screen', 'negative')]\n","output: 1\n","\n","Example 6\n","true aspect sentiments: [('battery life', 'positive')]\n","predicted aspect sentiments: [('battery life', 'positive'), ('battery', 'positive')]\n","output: 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbHIccjJUH1O"},"outputs":[],"source":["eval_prompt = PromptTemplate(\n","    input_variables=[\"true_aspect_sentiments\", \"predicted_aspect_sentiments\"],\n","    template=\"\"\"\n","You are given two lists of tuples. Each tuple consists of an aspect and a sentiment.\n","Your task is to determine how many tuples in the predicted aspect sentiments list closely match tuples in the true aspect sentiments list.\n","\n","To match:\n","1. First, you need to determine if the first entry (the aspect) in the tuples is describing the same thing. For example, \"battery\" and \"battery life\" are roughly describing the same thing, but \"operating system\" and \"packaging\" are not.\n","2. If the first entries match, then check if the second entries (the sentiment values) are the same. For example, \"positive\" matches \"positive\", but \"positive\" does not match \"neutral\".\n","3. Count each tuple in the true aspect sentiments list only if it has at least one match in the predicted aspect sentiments list.\n","\n","Please output the number of exact matches you find between the two lists. If no matches are found, output 0. Below are some examples:\n","\n","Example 1:\n","true aspect sentiments: [(suite of software, positive)]\n","predicted aspect sentiments: [(software, positive), (suite, positive)]\n","output: 1\n","\n","Example 2:\n","true aspect sentiments: [('space', 'positive'), ('keyboard', 'negative')]\n","predicted aspect sentiments: [('extra space', 'positive'), ('keyboard', 'negative')]\n","output: 2\n","\n","Example 3:\n","true aspect sentiments: [('price premium', 'negative'), ('features', 'positive')]\n","predicted aspect sentiments: [('price', 'neutral'), ('features', 'positive')]\n","output: 1\n","\n","Example 4:\n","true aspect sentiments: [('web cam', 'neutral'), (\"burn cd's\", 'neutral')]\n","predicted aspect sentiments: [('web cam', 'negative'), ('cd burning', 'negative')]\n","output: 0\n","\n","Example 5:\n","true aspect sentiments: [('space', 'positive'), ('keyboard', 'negative')]\n","predicted aspect sentiments: [('storage', 'positive'), ('screen', 'negative')]\n","output: 1\n","\n","Example 6:\n","true aspect sentiments: [('battery life', 'positive')]\n","predicted aspect sentiments: [('battery life', 'positive'), ('battery', 'positive')]\n","output: 1\n","\n","Here are the two lists:\n","\n","true aspect sentiments: {true_aspect_sentiments}\n","predicted aspect sentiments: {predicted_aspect_sentiments}\n","\n","Your output should be the number of matching tuples. Do not provide any additional words or explanations.\n","\n","output:\"\"\"\n",")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Q8mJwegpUH1O"},"source":["__prompt__ now, please include the prompt object (which should be named \"eval_prompt\") into a langchain chain with the llm object \"llm\" using the \"|\" operator. Please aslo add a final function to the chain that parses the output into an integer format. So, the chain should look like\n","\n","eval_chain = eval_prompt | llm | number_parser"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64ZBhFV9UH1O"},"outputs":[],"source":["def number_parser(output: str) -> int:\n","    try:\n","        # Parse the number from the output, ensuring it's an integer\n","        return int(output.strip())\n","    except ValueError:\n","        # In case the output is invalid, return 0 (or handle as needed)\n","        return 0\n","\n","# Build the eval_chain with eval_prompt, llm, and number_parser\n","eval_chain = eval_prompt | llm | number_parser\n","\n","# Example usage\n","output = eval_chain.invoke({\n","    \"true_aspect_sentiments\": [('price premium', 'negative'), ('features', 'positive')],\n","    \"predicted_aspect_sentiments\": [('price', 'neutral'), ('features', 'positive')]\n","})\n","\n","print(output)"]},{"cell_type":"markdown","metadata":{"id":"v_Vz79R5UH1O"},"source":["__prompt__ Now, please give me the code to use the eval_chain on each entry in the eval_df. For the \"true_aspect_sentiments\" take the values from the \"true_aspect_sentiment\" column and for the \"predicted_aspect_sentiments\" take the values from the aspect_sentiment. Please save the outputs in a new column called \"matches\". Please also use tqdm when iterating over the columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8sYiPHBDUH1O"},"outputs":[],"source":["tqdm.pandas()\n","\n","# Create a function to apply eval_chain on each row of the dataframe\n","def apply_eval_chain(row):\n","    true_aspect_sentiments = row['true_aspect_sentiment']\n","    predicted_aspect_sentiments = row['aspect_sentiment']\n","\n","    # Use the eval_chain to get the number of matches\n","    return eval_chain.invoke({\n","        \"true_aspect_sentiments\": true_aspect_sentiments,\n","        \"predicted_aspect_sentiments\": predicted_aspect_sentiments\n","    })\n","\n","eval_df['matches'] = eval_df.progress_apply(apply_eval_chain, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORD1_-lkUH1O"},"outputs":[],"source":["eval_df['matches'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"FBFKors7UH1P"},"source":["__prompt__ Now, please give me the code to get the number of tuples in each list in the 'true_aspect_sentiment' column of \"eval_df\" and save that to the column \"num_true_aspects\". Also do the same for the 'aspect_sentiment' and call the column \"num_pred_aspects\". Finally, using the counts in the 'matches' column and the \"num_true_aspects\" compute the difference between them and divide this result by the value in \"num_true_aspects\", then subtract this vaue from 1.0, and take the min of this value an 1.0 (i.e., there should never be a result larger than 1.0). Then, save this result in a new column called \"accuracy\"\n","\n","Finally, give the code for computing the statistics from the \"accuracy\" column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4J7m-B-tUH1P"},"outputs":[],"source":["# Compute the number of tuples in each list in 'true_aspect_sentiment' and 'aspect_sentiment' columns\n","eval_df['num_true_aspects'] = eval_df['true_aspect_sentiment'].apply(len)\n","eval_df['num_pred_aspects'] = eval_df['aspect_sentiment'].apply(len)\n","\n","# Compute the accuracy based on the formula\n","eval_df['accuracy'] = (\n","    1.0 - ((eval_df['num_true_aspects'] - eval_df['matches']) / eval_df['num_true_aspects'])\n",").clip(upper=1.0)  # Ensure accuracy does not exceed 1.0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgjjlsLsUH1P"},"outputs":[],"source":["# Compute basic statistics for the 'accuracy' column\n","accuracy_stats = eval_df['accuracy'].describe()\n","\n","# Optionally, compute additional statistics like mean, median, etc.\n","accuracy_mean = eval_df['accuracy'].mean()\n","accuracy_median = eval_df['accuracy'].median()\n","accuracy_std = eval_df['accuracy'].std()\n","\n","# Print the statistics\n","print(\"Accuracy Statistics:\")\n","print(accuracy_stats)\n","print(f\"Mean Accuracy: {accuracy_mean:.4f}\")\n","print(f\"Median Accuracy: {accuracy_median:.4f}\")\n","print(f\"Standard Deviation of Accuracy: {accuracy_std:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"1wtH9KQLUH1P"},"source":["__prompt__ finally, please give the python code to plot a histogram of the \"accuracy\" column of \"eval_df\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BZQ50RrUH1P"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Plot a histogram of the 'accuracy' column\n","plt.figure(figsize=(10, 6))\n","plt.hist(eval_df['accuracy'], bins=20, edgecolor='black', alpha=0.7)\n","plt.title('Distribution of Accuracy', fontsize=16)\n","plt.xlabel('Accuracy', fontsize=14)\n","plt.ylabel('Frequency', fontsize=14)\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"CwL6GeFtUH1P"},"source":["__prompt__ What would be a good way to measure performance in this scenario? I have the number of correct matches in \"matches\", the number of aspect sentiments that should have been produced in \"num_true_aspects\" and the number of predicted aspects in \"num_pred_aspects\". Ideally, the number of correct matches should exactly match \"num_true_aspects\", and \"num_true_aspects\" should exactly match \"num_pred_aspects\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOz7TLNUUH1X"},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# Calculate Precision, Recall, and F1 score\n","precision = eval_df['matches'].sum() / eval_df['num_pred_aspects'].sum()\n","recall = eval_df['matches'].sum() / eval_df['num_true_aspects'].sum()\n","f1 = 2 * (precision * recall) / (precision + recall)\n","\n","# Calculate Match Efficiency\n","match_efficiency = 1.0 - abs(eval_df['num_pred_aspects'].sum() - eval_df['num_true_aspects'].sum()) / eval_df['num_true_aspects'].sum()\n","\n","# Print the performance metrics\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")\n","print(f\"Match Efficiency: {match_efficiency:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNfvO4koUH1Y"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4cb236b211a84a57a9df9dc2b59ca915":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88a9ed5c779c44e39df501f8fab9984e","IPY_MODEL_fe57e5954cb9493f84b4842f810ec7b0","IPY_MODEL_280408967ec646ba9c9d765face6209d"],"layout":"IPY_MODEL_0b3713c2afd74a9fb23dfe153e33d9d0"}},"88a9ed5c779c44e39df501f8fab9984e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03a40de991a24cb5810ba52c82965df2","placeholder":"​","style":"IPY_MODEL_6fabeba2bfe442e794cd32426cd0e70e","value":"config.json: 100%"}},"fe57e5954cb9493f84b4842f810ec7b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c44de7799a4147798e608b587b6b966d","max":658,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d900cc81bc69430c8fad97c840cd29a8","value":658}},"280408967ec646ba9c9d765face6209d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60ed0dd5a20a435792dcb26f759dd064","placeholder":"​","style":"IPY_MODEL_9cd05ffce7ba48ed9713b8a7d0c66edf","value":" 658/658 [00:00&lt;00:00, 68.5kB/s]"}},"0b3713c2afd74a9fb23dfe153e33d9d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03a40de991a24cb5810ba52c82965df2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fabeba2bfe442e794cd32426cd0e70e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c44de7799a4147798e608b587b6b966d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d900cc81bc69430c8fad97c840cd29a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60ed0dd5a20a435792dcb26f759dd064":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cd05ffce7ba48ed9713b8a7d0c66edf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa0c6fe5371e4b84b76ff74d8dcc0b3d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_066ad96fe8e04757bfaf439aa08b99dd","IPY_MODEL_7f498a16eb764401977885e20d88349d","IPY_MODEL_95b30256ab61473fafb41ae7b754bd1e"],"layout":"IPY_MODEL_5a0f25d243704573a23889f813f73908"}},"066ad96fe8e04757bfaf439aa08b99dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb7356f4f28d4cc3acfef33112dbf604","placeholder":"​","style":"IPY_MODEL_b88eddfcfc0249e5a02876e9db25dba3","value":"model.safetensors.index.json: 100%"}},"7f498a16eb764401977885e20d88349d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a746ae7230b245018c4433b3d509a896","max":16519,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2ecb84dc40c4962a9c2b88e25e6f30b","value":16519}},"95b30256ab61473fafb41ae7b754bd1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_841606ef0aa54d7a9ee8f1e0e6506ac0","placeholder":"​","style":"IPY_MODEL_d6ecfcebe3df4689b44f4f28ec52b221","value":" 16.5k/16.5k [00:00&lt;00:00, 1.68MB/s]"}},"5a0f25d243704573a23889f813f73908":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb7356f4f28d4cc3acfef33112dbf604":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b88eddfcfc0249e5a02876e9db25dba3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a746ae7230b245018c4433b3d509a896":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2ecb84dc40c4962a9c2b88e25e6f30b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"841606ef0aa54d7a9ee8f1e0e6506ac0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6ecfcebe3df4689b44f4f28ec52b221":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc7176fcc2e7413e84f8d9ce94273ddd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7bfc3981288b451aa339fb6386d62c34","IPY_MODEL_a1ecc6d4cba34a89840a1aacdb8dddd8","IPY_MODEL_c8f4c01561ba4e47a35ef78348ab90b1"],"layout":"IPY_MODEL_45a3a00f6ca946c88b213f14bfab60b2"}},"7bfc3981288b451aa339fb6386d62c34":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_592e69d898734d74ab1dbcccc7a9a11f","placeholder":"​","style":"IPY_MODEL_622984569a3548629fd40740244bba2e","value":"Downloading shards: 100%"}},"a1ecc6d4cba34a89840a1aacdb8dddd8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a797df0a0844e8cb6d04358a1ee17ba","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ea03289b0c4482ba118d8b90b191643","value":2}},"c8f4c01561ba4e47a35ef78348ab90b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b8d1f66063e468f8b7564a3e43dc9a7","placeholder":"​","style":"IPY_MODEL_5290097e466f4d0fbfbd1ba8a697ed6d","value":" 2/2 [02:34&lt;00:00, 69.63s/it]"}},"45a3a00f6ca946c88b213f14bfab60b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"592e69d898734d74ab1dbcccc7a9a11f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"622984569a3548629fd40740244bba2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a797df0a0844e8cb6d04358a1ee17ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ea03289b0c4482ba118d8b90b191643":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b8d1f66063e468f8b7564a3e43dc9a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5290097e466f4d0fbfbd1ba8a697ed6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5defcb1bad746208822be2d5eaca551":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_904485f2c6be445b976642dedfa17191","IPY_MODEL_119d9d5686fa402f8c9b302a3d986d28","IPY_MODEL_74bab200d75c4b0d9c6b9225bb1e0d29"],"layout":"IPY_MODEL_d6e524297c794f019adfc97c2f6dcb89"}},"904485f2c6be445b976642dedfa17191":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd0e396baad74b32aee4e05c8de4ad32","placeholder":"​","style":"IPY_MODEL_7cdcd3800bbb4d72b13eaf0f828b8d2e","value":"model-00001-of-00002.safetensors: 100%"}},"119d9d5686fa402f8c9b302a3d986d28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca173a48b18a4126b767252f0a2aecb0","max":4989378032,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be64c4a5424849ef8b2af663db39a45e","value":4989378032}},"74bab200d75c4b0d9c6b9225bb1e0d29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e28434a1de6a48ddaeccaf1590748406","placeholder":"​","style":"IPY_MODEL_36601d9fb7394e3e893114d195ad4730","value":" 4.99G/4.99G [01:58&lt;00:00, 42.2MB/s]"}},"d6e524297c794f019adfc97c2f6dcb89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd0e396baad74b32aee4e05c8de4ad32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cdcd3800bbb4d72b13eaf0f828b8d2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca173a48b18a4126b767252f0a2aecb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be64c4a5424849ef8b2af663db39a45e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e28434a1de6a48ddaeccaf1590748406":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36601d9fb7394e3e893114d195ad4730":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa9c171a2310406eb5256d9fcf3f45a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1f0f3ab916e4c099b9e143ab4739ed7","IPY_MODEL_5dfba9dbb0fa4edcb49e21d44614e77e","IPY_MODEL_df6cb4a6a9654a58ab2ca53975f19cb7"],"layout":"IPY_MODEL_797d0b7040d54cf790793ea1f4a3cfbf"}},"e1f0f3ab916e4c099b9e143ab4739ed7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03af709f35044e69b3c3b1bf25e1e21c","placeholder":"​","style":"IPY_MODEL_3a58ba2889fc44d5940cd44d94af26ee","value":"model-00002-of-00002.safetensors: 100%"}},"5dfba9dbb0fa4edcb49e21d44614e77e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_738228c6c1fb45a1a3cb8f43d9ecbcef","max":1465955608,"min":0,"orientation":"horizontal","style":"IPY_MODEL_01bf8028ff9248efad7933fb51b3baf3","value":1465955608}},"df6cb4a6a9654a58ab2ca53975f19cb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4045ba6f7fda488ab54b378a41a23f5e","placeholder":"​","style":"IPY_MODEL_ef612e194fc54dd99fc5a465b25146bf","value":" 1.47G/1.47G [00:34&lt;00:00, 42.3MB/s]"}},"797d0b7040d54cf790793ea1f4a3cfbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03af709f35044e69b3c3b1bf25e1e21c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a58ba2889fc44d5940cd44d94af26ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"738228c6c1fb45a1a3cb8f43d9ecbcef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01bf8028ff9248efad7933fb51b3baf3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4045ba6f7fda488ab54b378a41a23f5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef612e194fc54dd99fc5a465b25146bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c7fce0ca53d44b9902cf6a5abfe7581":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f689d078f13482280c1feb7822a1a12","IPY_MODEL_44a38777069d4e9e8d82ee8611d406b0","IPY_MODEL_71952ec84dbe46e38eaa446d98d74f6e"],"layout":"IPY_MODEL_e2cc8ca3212e492b8e890d4c22b41066"}},"9f689d078f13482280c1feb7822a1a12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c98d1c4cb71a494697cdf9c5565c00a4","placeholder":"​","style":"IPY_MODEL_617260252b0b4dd09a36a5496ac59dfe","value":"Loading checkpoint shards:  50%"}},"44a38777069d4e9e8d82ee8611d406b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e5d81217ae74e39917ec20873a91648","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c96fa41a542e488da9ee8b9e3bed1476","value":1}},"71952ec84dbe46e38eaa446d98d74f6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c9c152afec34198a3f8dc57ad38e8f3","placeholder":"​","style":"IPY_MODEL_1fc004033cbe41fb88e89cbb929043fc","value":" 1/2 [00:22&lt;00:22, 22.17s/it]"}},"e2cc8ca3212e492b8e890d4c22b41066":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c98d1c4cb71a494697cdf9c5565c00a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"617260252b0b4dd09a36a5496ac59dfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e5d81217ae74e39917ec20873a91648":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c96fa41a542e488da9ee8b9e3bed1476":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c9c152afec34198a3f8dc57ad38e8f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fc004033cbe41fb88e89cbb929043fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}