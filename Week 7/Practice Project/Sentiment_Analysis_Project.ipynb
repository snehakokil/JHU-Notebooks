{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snehakokil/JHU-Notebooks/blob/main/Sentiment_Analysis_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L8Gpbp5w2a-w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyTorch\n",
            "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: PyTorch\n",
            "  Building wheel for PyTorch (pyproject.toml) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for PyTorch \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[23 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
            "  \u001b[31m   \u001b[0m     main()\n",
            "  \u001b[31m   \u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
            "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
            "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 280, in build_wheel\n",
            "  \u001b[31m   \u001b[0m     return _build_backend().build_wheel(\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/lx/sf3nk2f91gdg4grbl_m81_y00000gn/T/pip-build-env-9svw9qq1/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 435, in build_wheel\n",
            "  \u001b[31m   \u001b[0m     return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/lx/sf3nk2f91gdg4grbl_m81_y00000gn/T/pip-build-env-9svw9qq1/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 423, in _build\n",
            "  \u001b[31m   \u001b[0m     return self._build_with_temp_dir(\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/lx/sf3nk2f91gdg4grbl_m81_y00000gn/T/pip-build-env-9svw9qq1/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 404, in _build_with_temp_dir\n",
            "  \u001b[31m   \u001b[0m     self.run_setup()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/lx/sf3nk2f91gdg4grbl_m81_y00000gn/T/pip-build-env-9svw9qq1/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 512, in run_setup\n",
            "  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/lx/sf3nk2f91gdg4grbl_m81_y00000gn/T/pip-build-env-9svw9qq1/overlay/lib/python3.12/site-packages/setuptools/build_meta.py\", line 317, in run_setup\n",
            "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
            "  \u001b[31m   \u001b[0m   File \"<string>\", line 15, in <module>\n",
            "  \u001b[31m   \u001b[0m Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for PyTorch\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build PyTorch\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (PyTorch)\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: langchain in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (0.4.27)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: langchain-huggingface in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.3.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-huggingface) (0.3.76)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-huggingface) (0.22.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-huggingface) (0.34.4)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.27)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (24.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.10)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.24.0)\n",
            "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.9.0)\n",
            "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.7)\n",
            "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2.2.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "hello world\n"
          ]
        }
      ],
      "source": [
        "#!pip install pandas matplotlib scikit-learn numpy nltk\n",
        "#!pip install --upgrade nltk\n",
        "#!pip install transformers\n",
        "#!pip install --upgrade datasets\n",
        "#!pip install --upgrade huggingface_hub\n",
        "! pip install PyTorch    \n",
        "! pip install langchain\n",
        "! pip install transformers\n",
        "! pip install langchain-huggingface\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "# for NLP tasks\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "# for prompt engineering\n",
        "\n",
        "\n",
        "print(\"hello world\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Load and Explore the Dataset\n",
        "The given dataset is in a text file. I converted the text file into CSV for easier processing and loaded into a Pandas Dataframe. Next, I cleaned the dataset by filling in the empty values with a blank string. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the sentiment analysis dataset from a .txt file and save it as a .csv file\n",
        "# Clean the data and handle potential parsing issues\n",
        "# Path to your input .txt file\n",
        "input_file = \"Product_Sentiment.txt\"\n",
        "output_file = \"Product_Sentiment.csv\"\n",
        "\n",
        "data = []\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip().rstrip(\",\")  # remove trailing comma\n",
        "        if line:  # skip empty lines\n",
        "            try:\n",
        "                # Safely evaluate the tuple string into a Python tuple\n",
        "                text, label = ast.literal_eval(line)\n",
        "                data.append((text, label))\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping line due to error: {line} -> {e}\")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"text\", \"label\"])\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Dataset loaded and saved as CSV!\")\n",
        "print(df.head())\n",
        "\n",
        "# read the csv file and clean the data to remove any rows with missing or null values\n",
        "df = pd.read_csv(\"Product_Sentiment.csv\")\n",
        "df.fillna('', inplace=True)\n",
        "df.reset_index(drop=True, inplace=True) # reset index after dropping rows   \n",
        "print(\"✅ Data cleaned!\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Build a Traditional ML Classifier\n",
        "For this we first need to split data into training and testing. From the existing dataset, I chose to reserve 80% of the data for training and 20% for testing. \n",
        "\n",
        "### Prepare training and testing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your dataset (CSV with columns: \"text\", \"label\")\n",
        "df = pd.read_csv(\"Product_Sentiment.csv\")\n",
        "\n",
        "# Split into train (80%) and test (20%)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "\n",
        "# Save to CSV files\n",
        "train_df.to_csv(\"train.csv\", index=False, encoding=\"utf-8\")\n",
        "test_df.to_csv(\"test.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Dataset successfully split!\")\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Test size:\", len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create SVM classifier and train. Then apply the test data\n",
        "(Used ChatGPT to understand steps)\n",
        "This task requires building a pipeline, where the text data is converted into numerical features and then passed onto SVM classifier training and testing functions.\n",
        "\n",
        "To convert text into numerical features, ChatGPT recommended using the TF-IDF Vectorizer. It also suggested the use of base classifier LinearSVC, which is known for its text classification capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load train and test datasets\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Build a pipeline: TF-IDF vectorizer + Linear SVM\n",
        "# svm_pipeline = Pipeline([\n",
        "#    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "#    ('svm', LinearSVC(random_state=42))\n",
        "# ])\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Product_Sentiment.csv\")\n",
        "\n",
        "# Define stratified k-fold (preserves class balance)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# CountVectorizer can also be used instead of TfidfVectorizer\n",
        "svm_pipeline = Pipeline([\n",
        "     ('count', CountVectorizer(stop_words='english')),\n",
        "     ('svm', LinearSVC(random_state=42))\n",
        "])\n",
        "\n",
        "svm_scores = cross_val_score(svm_pipeline, df['text'], df['label'], cv=kfold, scoring='accuracy')\n",
        "print(\"SVM + BoW - Accuracy per fold:\", svm_scores)\n",
        "print(\"SVM + BoW - Mean Accuracy:\", svm_scores.mean())\n",
        "\n",
        "# Train the model\n",
        "svm_pipeline.fit(df['text'], df['label'])\n",
        "\n",
        "# Evaluate on test set\n",
        "#y_pred = svm_pipeline.predict(test_df['text'])\n",
        "#print(\"✅ Accuracy:\", accuracy_score(test_df['label'], y_pred))\n",
        "#print(\"\\nClassification Report:\\n\", classification_report(test_df['label'], y_pred))\n",
        "\n",
        "# Example prediction\n",
        "example = [\"I really love this new phone!\", \"This is the worst product I’ve ever bought.\"]\n",
        "predictions = svm_pipeline.predict(example)\n",
        "for text, label in zip(example, predictions):\n",
        "    print(f\"Text: {text} -> Predicted Sentiment: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of SVM with TF-IDF vectorizer\n",
        "Looking at the results, especially the precision score of 0.00, the model performed poorly on picking up positive sentiments, while it did a bit better on negative ones. In spite of the performance scores, the predictions it generated on examples were accurate.\n",
        "\n",
        "When analyzed with the help of ChatGPT, I understood that the TF-IDF vectorizer usually works better with larger datasets. This dataset being very small, became unstable and could not represent the positive sentiment test data very well.\n",
        "\n",
        "ChatGPT also suggested using cross-validation to improve accuracy score on a small dataset, instead of 80-20 split. Another option was to try using simpler models like Naive Bayes.\n",
        "\n",
        "I will be first trying to improve performance with Bag of Words approach and then test the suggestions given above.\n",
        "\n",
        "### Analysis of SVM with Count Vectorizer\n",
        "When replaced with this to implement Bag of Words approach, the accuracy score went up a little bit (50% from 37%), recall improved to 80%, because of smaller, imbalanced dataset - which means the model will rarely miss negative sentiments, sometimes even considering a positive sentiment as negative.\n",
        "\n",
        "### Alterntive: Naive Bayes\n",
        "Let's see how this model works. \n",
        "\n",
        "when ran this model with Bag of Words, it still gave out 37% accuracy, which was no improvement from the previous SVM implementation. In fact, compared to this, SVM with BOW gave better accuracy. (same as SVM + TF-IDF, and worse than SVM + BoW at 50%)\n",
        "\n",
        "### Adding k-fold cross validation\n",
        "Using this, I am trying to balance the train and test datasets in the hopes to get better precision and accuracy, if possible.\n",
        "\n",
        "For this, I modified the SVM + BoW implementation above.\n",
        "\n",
        "This approach improved the mean accuracy over 5 k-folds to *67%*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load train and test datasets\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# CountVectorizer can also be used instead of TfidfVectorizer\n",
        "nb_pipeline = Pipeline([\n",
        "     ('count', CountVectorizer(stop_words='english')),\n",
        "     ('nb', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "nb_pipeline.fit(train_df['text'], train_df['label'])\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = nb_pipeline.predict(test_df['text'])\n",
        "print(\"✅ Accuracy:\", accuracy_score(test_df['label'], y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(test_df['label'], y_pred))\n",
        "\n",
        "# Example prediction\n",
        "example = [\"I really love this new phone!\", \"This is the worst product I’ve ever bought.\"]\n",
        "predictions = nb_pipeline.predict(example)\n",
        "for text, label in zip(example, predictions):\n",
        "    print(f\"Text: {text} -> Predicted Sentiment: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Experiment with a Traditional NLP Approach\n",
        "To use NLP techniques, I first need to pre-process the data to tokenize it, remove stopwords from it and then apply lemmatization. For that, the *nltk* library functions are used.\n",
        "\n",
        "The following block contains functions supporting the data pre-processing done later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialising Stopwords and Lemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from a list of tokenized words\"\"\"\n",
        "    return [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in words]\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all words to lowercase\"\"\"\n",
        "    return [word.lower() for word in words]\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation\"\"\"\n",
        "    return [re.sub(r'[^\\w\\s]', '', word) for word in words if re.sub(r'[^\\w\\s]', '', word) != '']\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stopwords\"\"\"\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "def lemmatize_list(words):\n",
        "    \"\"\"Lemmatize words\"\"\"\n",
        "    return [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "def normalize(text):\n",
        "    \"\"\"Full preprocessing pipeline\"\"\"\n",
        "    words = nltk.word_tokenize(text)  # Tokenize text\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_stopwords(words)\n",
        "    words = lemmatize_list(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Load your dataset (CSV with columns: \"text\", \"label\")\n",
        "df = pd.read_csv(\"sample_data/Product_Sentiment.csv\")\n",
        "\n",
        "# Apply normalization to the text data\n",
        "df['cleaned_text'] = df['text'].apply(normalize)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sentiment Analysis\n",
        "\n",
        "Outcome expected:\n",
        "1. Total sentiment score\n",
        "2. % of positive and negative sentiments\n",
        "\n",
        "Like we did in our MLS exercises, I will be using VADER - rule-based sentiment analysis tool from nltk.sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download VADER lexicon if not already downloaded\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# this method gets the compound scores, places thresholds to classify them into positive, negative and neutral\n",
        "def get_sentiment_score(text):\n",
        "    sentiment_scores = sia.polarity_scores(text)\n",
        "    if sentiment_scores['compound'] >=0.05 and sentiment_scores['pos'] > sentiment_scores['neg']:\n",
        "        return 'positive'\n",
        "    elif sentiment_scores['compound'] <= -0.05 and sentiment_scores['neg'] > sentiment_scores['pos']:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# Apply sentiment analysis\n",
        "df[\"sentiment\"] = df[\"cleaned_text\"].apply(get_sentiment_score)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization\n",
        "I explored the word cloud to visualize the bigger words that signify positive sentiments about the product. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example: Assume df is already loaded and has columns ['text', 'sentiment']\n",
        "# df = pd.read_csv(\"your_dataset.csv\")\n",
        "\n",
        "# Filter positive sentiment rows\n",
        "positive_texts = df[df['sentiment'] == 'positive']['text']\n",
        "\n",
        "# Join all positive texts into one string\n",
        "positive_words = \" \".join(positive_texts.astype(str))\n",
        "\n",
        "# Generate the wordcloud\n",
        "wordcloud = WordCloud(width=800,\n",
        "                      height=400,\n",
        "                      background_color=\"white\",\n",
        "                      colormap=\"Greens\",\n",
        "                      max_words=200).generate(positive_words)\n",
        "\n",
        "# Visualize the wordcloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud of Positive Sentiment Words\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Try a Transformer-Based Model\n",
        "In this task, I tried using the transformers and datasets from HuggingFace. For the use case - Sentiment Analysis, it is more appropriate to use encoder-only model like BERT-base that understands and processes given text and outputs the analysis from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"sample_data/Product_Sentiment.csv\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create pipeline for sentiment analysis\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_all_scores=True,   # capture all label probabilities\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    device=0 if torch.cuda.is_available() else -1  # use GPU if available\n",
        ")\n",
        "\n",
        "# Run sentiment analysis\n",
        "results = sentiment_pipeline(df[\"text\"].tolist())\n",
        "\n",
        "# Extract sentiment and scores\n",
        "labels = []\n",
        "scores = []\n",
        "\n",
        "for res in results:\n",
        "    # res is a list of dicts with 'label' and 'score'\n",
        "    # Pick the label with max score\n",
        "    best = max(res, key=lambda x: x['score'])\n",
        "    labels.append(best['label'])\n",
        "    scores.append({r['label']: r['score'] for r in res})\n",
        "\n",
        "# Add results to dataframe\n",
        "df[\"sentiment\"] = labels\n",
        "df[\"sentiment_scores\"] = scores\n",
        "\n",
        "\n",
        "# Save to new CSV (optional)\n",
        "df.to_csv(\"Product_Sentiment_Analyzed.csv\", index=False)\n",
        "df.head()\n",
        "\n",
        "# count the number of positive, negative and neutral sentiments\n",
        "product_sentiment_count = df['sentiment'].value_counts()\n",
        "print(product_sentiment_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5: Explore Prompt Engineering\n",
        "I used ChatGPT to come up with the code that uses openai, Llama models and comapres the outcomes of few-shot and chain of thought prompts. I explored the use if LangChain pipeline to define prompt templates, llm layers and chain them for outcome. \n",
        "\n",
        "I could not compare outcomes at this time due to API rate limiting and quota on my personal plans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! pip install langchain_community\n",
        "! pip install --upgrade openai\n",
        "import pandas as pd\n",
        "from langchain.prompts import PromptTemplate\n",
        "#from langchain.llms import OpenAI # Old import\n",
        "from langchain_community.chat_models import ChatOpenAI # New import for chat models\n",
        "# If you want to use Hugging Face (like LLaMA), import:\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain_core.messages import HumanMessage # Import HumanMessage\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "# Set an environment variable\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"sample_data/Product_Sentiment.csv\")\n",
        "\n",
        "# ---- Choose your LLM ----\n",
        "# Option A: OpenAI\n",
        "# llm = OpenAI(model_name=\"gpt-4o-mini\", temperature=0) # Old way\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0) # New way for chat models\n",
        "\n",
        "# Option B: HuggingFace (LLaMA)\n",
        "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "# hf_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128)\n",
        "# llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "# ---- Few-shot Prompt ----\n",
        "few_shot_template = \"\"\"\n",
        "You are a sentiment analysis assistant.\n",
        "Classify the following product review as Positive, Negative, or Neutral.\n",
        "\n",
        "Examples:\n",
        "Review: \"I love this product! It works perfectly.\" → Positive\n",
        "Review: \"The item arrived broken and useless.\" → Negative\n",
        "Review: \"It's okay, nothing special.\" → Neutral\n",
        "\n",
        "Now classify:\n",
        "Review: \"{review}\"\n",
        "Answer:\n",
        "\"\"\"\n",
        "few_shot_prompt = PromptTemplate(template=few_shot_template, input_variables=[\"review\"])\n",
        "\n",
        "# ---- Chain-of-Thought Prompt ----\n",
        "cot_template = \"\"\"\n",
        "You are a sentiment analysis assistant.\n",
        "Think step by step and explain your reasoning before giving the final classification\n",
        "as Positive, Negative, or Neutral.\n",
        "\n",
        "Review: \"{review}\"\n",
        "\n",
        "First explain your reasoning.\n",
        "Then give the final answer in this format:\n",
        "Final Answer: <Positive/Negative/Neutral>\n",
        "\"\"\"\n",
        "cot_prompt = PromptTemplate(template=cot_template, input_variables=[\"review\"])\n",
        "\n",
        "\n",
        "# ---- Functions to run prompts ----\n",
        "def few_shot_sentiment(text):\n",
        "    # Pass a list containing a HumanMessage object to the llm\n",
        "    return llm([HumanMessage(content=few_shot_prompt.format(review=text))]).content.strip()\n",
        "\n",
        "def chain_of_thought_sentiment(text):\n",
        "    # Pass a list containing a HumanMessage object to the llm\n",
        "    return llm([HumanMessage(content=cot_prompt.format(review=text))]).content.strip()\n",
        "\n",
        "# ---- Apply both approaches ----\n",
        "df[\"few_shot_sentiment\"] = df[\"text\"].apply(few_shot_sentiment)\n",
        "df[\"cot_sentiment\"] = df[\"text\"].apply(chain_of_thought_sentiment)\n",
        "\n",
        "# Save results\n",
        "df.to_csv(\"Product_Sentiment_LangChain.csv\", index=False)\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations and Insights\n",
        "\n",
        "1. The use of basic ML models and classifiers required more mathematical approaches than the NLP and LLMs used later. \n",
        "2. Basic ML models also required us to do the training, while for the LLMs we used pre-trained models.\n",
        "3. Sophisticated prompts and prompt engineering techniques should help derive a lot more information from large set of inputs.\n",
        "4. Through this exercise I was able to explore and understand in detail about workings on various classifiers, models, how they train and produce outcomes and the strategies taught in the weekly classes."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN8juxLJryT+9lAkKtFFYVP",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
