{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snehakokil/JHU-Notebooks/blob/main/Sentiment_Analysis_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8Gpbp5w2a-w"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas matplotlib scikit-learn numpy\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "print(\"hello world\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Load and Explore the Dataset\n",
        "The given dataset is in a text file. I converted the text file into CSV for easier processing and loaded into a Pandas Dataframe. Next, I cleaned the dataset by filling in the empty values with a blank string. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the sentiment analysis dataset from a .txt file and save it as a .csv file\n",
        "# Clean the data and handle potential parsing issues\n",
        "# Path to your input .txt file\n",
        "input_file = \"Product_Sentiment.txt\"\n",
        "output_file = \"Product_Sentiment.csv\"\n",
        "\n",
        "data = []\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip().rstrip(\",\")  # remove trailing comma\n",
        "        if line:  # skip empty lines\n",
        "            try:\n",
        "                # Safely evaluate the tuple string into a Python tuple\n",
        "                text, label = ast.literal_eval(line)\n",
        "                data.append((text, label))\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping line due to error: {line} -> {e}\")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"text\", \"label\"])\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Dataset loaded and saved as CSV!\")\n",
        "print(df.head())\n",
        "\n",
        "# read the csv file and clean the data to remove any rows with missing or null values\n",
        "df = pd.read_csv(\"Product_Sentiment.csv\")\n",
        "df.fillna('', inplace=True)\n",
        "df.reset_index(drop=True, inplace=True) # reset index after dropping rows   \n",
        "print(\"✅ Data cleaned!\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Build a Traditional ML Classifier\n",
        "For this we first need to split data into training and testing. From the existing dataset, I chose to reserve 80% of the data for training and 20% for testing. \n",
        "\n",
        "### Prepare training and testing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your dataset (CSV with columns: \"text\", \"label\")\n",
        "df = pd.read_csv(\"Product_Sentiment.csv\")\n",
        "\n",
        "# Split into train (80%) and test (20%)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "\n",
        "# Save to CSV files\n",
        "train_df.to_csv(\"train.csv\", index=False, encoding=\"utf-8\")\n",
        "test_df.to_csv(\"test.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Dataset successfully split!\")\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Test size:\", len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create SVM classifier and train. Then apply the test data\n",
        "(Used ChatGPT to understand steps)\n",
        "This task requires building a pipeline, where the text data is converted into numerical features and then passed onto SVM classifier training and testing functions.\n",
        "\n",
        "To convert text into numerical features, ChatGPT recommended using the TF-IDF Vectorizer. It also suggested the use of base classifier LinearSVC, which is known for its text classification capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load train and test datasets\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Build a pipeline: TF-IDF vectorizer + Linear SVM\n",
        "# svm_pipeline = Pipeline([\n",
        "#    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "#    ('svm', LinearSVC(random_state=42))\n",
        "# ])\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Product_Sentiment.csv\")\n",
        "\n",
        "# Define stratified k-fold (preserves class balance)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# CountVectorizer can also be used instead of TfidfVectorizer\n",
        "svm_pipeline = Pipeline([\n",
        "     ('count', CountVectorizer(stop_words='english')),\n",
        "     ('svm', LinearSVC(random_state=42))\n",
        "])\n",
        "\n",
        "svm_scores = cross_val_score(svm_pipeline, df['text'], df['label'], cv=kfold, scoring='accuracy')\n",
        "print(\"SVM + BoW - Accuracy per fold:\", svm_scores)\n",
        "print(\"SVM + BoW - Mean Accuracy:\", svm_scores.mean())\n",
        "\n",
        "# Train the model\n",
        "svm_pipeline.fit(df['text'], df['label'])\n",
        "\n",
        "# Evaluate on test set\n",
        "#y_pred = svm_pipeline.predict(test_df['text'])\n",
        "#print(\"✅ Accuracy:\", accuracy_score(test_df['label'], y_pred))\n",
        "#print(\"\\nClassification Report:\\n\", classification_report(test_df['label'], y_pred))\n",
        "\n",
        "# Example prediction\n",
        "example = [\"I really love this new phone!\", \"This is the worst product I’ve ever bought.\"]\n",
        "predictions = svm_pipeline.predict(example)\n",
        "for text, label in zip(example, predictions):\n",
        "    print(f\"Text: {text} -> Predicted Sentiment: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis of SVM with TF-IDF vectorizer\n",
        "Looking at the results, especially the precision score of 0.00, the model performed poorly on picking up positive sentiments, while it did a bit better on negative ones. In spite of the performance scores, the predictions it generated on examples were accurate.\n",
        "\n",
        "When analyzed with the help of ChatGPT, I understood that the TF-IDF vectorizer usually works better with larger datasets. This dataset being very small, became unstable and could not represent the positive sentiment test data very well.\n",
        "\n",
        "ChatGPT also suggested using cross-validation to improve accuracy score on a small dataset, instead of 80-20 split. Another option was to try using simpler models like Naive Bayes.\n",
        "\n",
        "I will be first trying to improve performance with Bag of Words approach and then test the suggestions given above.\n",
        "\n",
        "### Analysis of SVM with Count Vectorizer\n",
        "When replaced with this to implement Bag of Words approach, the accuracy score went up a little bit (50% from 37%), recall improved to 80%, because of smaller, imbalanced dataset - which means the model will rarely miss negative sentiments, sometimes even considering a positive sentiment as negative.\n",
        "\n",
        "### Alterntive: Naive Bayes\n",
        "Let's see how this model works. \n",
        "\n",
        "when ran this model with Bag of Words, it still gave out 37% accuracy, which was no improvement from the previous SVM implementation. In fact, compared to this, SVM with BOW gave better accuracy. (same as SVM + TF-IDF, and worse than SVM + BoW at 50%)\n",
        "\n",
        "### Adding k-fold cross validation\n",
        "Using this, I am trying to balance the train and test datasets in the hopes to get better precision and accuracy, if possible.\n",
        "\n",
        "For this, I modified the SVM + BoW implementation above.\n",
        "\n",
        "This approach improved the mean accuracy over 5 k-folds to *67%*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load train and test datasets\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# CountVectorizer can also be used instead of TfidfVectorizer\n",
        "nb_pipeline = Pipeline([\n",
        "     ('count', CountVectorizer(stop_words='english')),\n",
        "     ('nb', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "nb_pipeline.fit(train_df['text'], train_df['label'])\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = nb_pipeline.predict(test_df['text'])\n",
        "print(\"✅ Accuracy:\", accuracy_score(test_df['label'], y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(test_df['label'], y_pred))\n",
        "\n",
        "# Example prediction\n",
        "example = [\"I really love this new phone!\", \"This is the worst product I’ve ever bought.\"]\n",
        "predictions = nb_pipeline.predict(example)\n",
        "for text, label in zip(example, predictions):\n",
        "    print(f\"Text: {text} -> Predicted Sentiment: {label}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN8juxLJryT+9lAkKtFFYVP",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
