{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyS2Ovli5gq2"
      },
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "In this notebook, we will demonstrate the fundementals of using LangChain for prompt engineering. Specifically, we will do the following:\n",
        "\n",
        "- create a prompt from a template\n",
        "- create a LLM\n",
        "- create a chain\n",
        "- look at some specialized chains for few-shot prompting and for Chain-Of-Thought\n",
        "\n",
        "For this  exercise, we are going to focus on a classification task. Namely, the classification of the \"stance\" of a comment towards another comment. The base comment is given below:\n",
        "\n",
        "```python\n",
        "comment = \"The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\"\n",
        "```\n",
        "\n",
        "The replies to the comment that we will classify for their stance toward the comment as \"agree\", \"disagree\", and \"neutral\" are:\n",
        "\n",
        "```python\n",
        "replies = [\n",
        "    \"The newer ones fail to live up to the sophistry of the older movies from the 70's.\",\n",
        "    \"Frank Herbert wrote a lot of books.\",\n",
        "    \"I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\",\n",
        "    \"The quick red fox jumped over the lazy brown dog.\",\n",
        "    \"Yeah, this new movie is a real masterpiece, lol!!\"\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--I4xWYW5gq7"
      },
      "source": [
        "## Configure the environment\n",
        "\n",
        "I need your help with classifying the stance of replies to comments about a topic using LangChain and Langchain-Huggingface for running local models. First, I need the code to install the neccesary packages from a notebook envrionment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RowDP8ff5gq8",
        "outputId": "5acd83cf-93c8-47a9-e414-bb9d19245bf8"
      },
      "outputs": [],
      "source": [
        "! pip install langchain\n",
        "! pip install transformers\n",
        "! pip install langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3hJLyfM5gq-"
      },
      "source": [
        "## Create a prompt object\n",
        "\n",
        "I need your help with classifying the stance of comments using LangChain. First, I need you to give me the code to create a prompt object, called \"stance_prompt\" from LangChain around the following template:\n",
        "'''Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words after outputing the label.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "stance:'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NU4fDbQB5gq_",
        "outputId": "dac58bc9-1e60-4d91-9866-ca7c1e850eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words, only the label.\n",
            "comment: I think this policy is not effective.\n",
            "reply: I agree, it doesn't address the core issues.\n",
            "stance:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define the template for stance classification\n",
        "template = '''Please classify the stance, or opinion, of the following reply to the comment. Note that we want the stance of the reply to the comment, and not the stance of the reply to topic of the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words, only the label.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "stance:\n",
        "'''\n",
        "\n",
        "# Create the prompt object\n",
        "stance_prompt = PromptTemplate(\n",
        "    input_variables=[\"comment\", \"reply\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# Example usage of the prompt\n",
        "comment = \"I think this policy is not effective.\"\n",
        "reply = \"I agree, it doesn't address the core issues.\"\n",
        "prompt = stance_prompt.format(comment=comment, reply=reply)\n",
        "print(prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSSsDeKQ5gq_"
      },
      "source": [
        "## Create an LLM object\n",
        "\n",
        "### Option 1: Using a small encoder-decoder model\n",
        "Now, I need you to create an LLM object using LangChain. In particular, I would like to use the text2text-generation model of \"declare-lab/flan-alpaca-gpt4-xl\" from HuggingFace and use the CPU. Make sure to import the langchain HuggingFace pipeline as \"from langchain_huggingface import HuggingFacePipeline\". Also, make sure when creating the pipeline to specify \"max_new_tokens = 500\", and make sure the pipeline only outputs the generated text and not text from the prompt.\n",
        "\n",
        "```python\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the model using Hugging Face pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"declare-lab/flan-alpaca-gpt4-xl\",\n",
        "    device=0,  # Use GPU (-1 for CPU)\n",
        "    max_new_tokens = 500,\n",
        ")\n",
        "\n",
        "# Create the LangChain LLM using the HuggingFace pipeline\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "# Example usage with the prompt object from before\n",
        "prompt = '''Please classify the stance, or opinion, of the following reply to the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words.\n",
        "comment: I think the new policy will help improve efficiency.\n",
        "reply: I disagree, the policy doesn't address the real issues.\n",
        "stance:'''\n",
        "\n",
        "# Get the model's response\n",
        "response = llm(prompt)\n",
        "print(response)\n",
        "```\n",
        "\n",
        "### Option 2: Using a small decoder-only model\n",
        "Now, I need you to create an LLM object using LangChain. In particular, I would like to use the text-generation model of \"tiiuae/Falcon3-1B-Instruct\" from HuggingFace and use the CPU. Make sure to import the langchain HuggingFace pipeline as \"from langchain_huggingface import HuggingFacePipeline\". Also, make sure when creating the pipeline to specify \"max_new_tokens = 500\", and make sure the pipeline only outputs the generated text and not the prompt.\n",
        "\n",
        "```python\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"tiiuae/Falcon3-1B-Instruct\",\n",
        "    device=0,  # Use GPU (-1 for CPU)\n",
        "    max_new_tokens = 500,\n",
        "    return_full_text=False\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "57503e437a0947068985d92302bb5a99",
            "090f0fb4b5b043df91280f4a05f2a110",
            "4dd918f565a446fa9b8e77b87a9a56fa",
            "f6ed95119f5242ae900c56c0b14dafbc",
            "af778a7c76aa4f8ea1c190394667d8e9",
            "9db64bbd996c4b1eb55346c051848225",
            "ac1dec2fcbd848578e9c87ca57040b30",
            "ed0ff2bb755f41ef9070e8ee0af0b7b3",
            "832e195a263048909254c5ccd87e8c62",
            "2079f966cb1348f29ce87ab007e79eaa"
          ]
        },
        "id": "GcJxbbQG5gq_",
        "outputId": "9d6c565e-a370-400f-a67b-aedb54172d89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 32.37it/s]\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " disagree\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the model using Hugging Face pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"tiiuae/Falcon3-3B-Instruct\",\n",
        "    device=-1,  # Use GPU (-1 for CPU)\n",
        "    max_new_tokens = 500,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "# Create the LangChain LLM using the HuggingFace pipeline\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "# Example usage with the prompt object from before\n",
        "prompt = '''Please classify the stance, or opinion, of the following reply to the comment. Only give the stance as \"agree\", \"disagree\", or \"neutral\" and output no other words.\n",
        "comment: I think the new policy will help improve efficiency.\n",
        "reply: I disagree, the policy doesn't address the real issues.\n",
        "stance:'''\n",
        "\n",
        "# Get the model's response\n",
        "response = llm(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h0G4UPb5grA"
      },
      "source": [
        "## Create a Chain\n",
        "\n",
        "Now, I would like the python code to create a LangChain Chain from the prompt template \"stance_prompt\" and the LLM \"llm\". Make sure to use the \"|\" syntax for defining the chain. and call the chain by the \"invoke\" method. Please name the chain \"stance_chain\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "W18QHavF5grB",
        "outputId": "3e140af9-82f9-444c-f8ac-cbd99222c1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|assistant|>\n",
            "agree\n"
          ]
        }
      ],
      "source": [
        "stance_chain = stance_prompt | llm\n",
        "\n",
        "# Example usage: run the chain with the provided comment and reply\n",
        "comment = \"I think the government should invest more in public health.\"\n",
        "reply = \"I agree that public health should be a priority.\"\n",
        "\n",
        "# Format the input and get the result\n",
        "result = stance_chain.invoke({\"comment\": comment, \"reply\": reply})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgrinvs_5grC"
      },
      "source": [
        "Great. Now, I would like to code to run the previously defined \"stance_chain\" on a comment called \"test_comment\" across each entry in a list called \"test_replies\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "x9tSvtUB5grC"
      },
      "outputs": [],
      "source": [
        "test_comment = \"The new Dune movie does not really capture the vision laid out by Frank Herbert. It feels like they tried to import too many visual effects that take away from the philosophy of the work.\"\n",
        "\n",
        "test_replies = [\n",
        "    \"The newer ones fail to live up to the sophistry of the older movies from the 70's.\",\n",
        "    \"Frank Herbert wrote a lot of books.\",\n",
        "    \"I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\",\n",
        "    \"The quick red fox jumped over the lazy brown dog.\",\n",
        "    \"Yeah, this new movie is a real masterpiece, lol!!\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "eHwYueMu5grC",
        "outputId": "3237fb68-38de-4b1f-f089-33c5b5f73598"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m responses = []\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m reply \u001b[38;5;129;01min\u001b[39;00m test_replies:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     response = \u001b[43mstance_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcomment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_comment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreply\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreply\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     responses.append(response)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3245\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3243\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3244\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3245\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3246\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3247\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:390\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    381\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    386\u001b[39m     **kwargs: Any,\n\u001b[32m    387\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    388\u001b[39m     config = ensure_config(config)\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    401\u001b[39m         .text\n\u001b[32m    402\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:789\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    782\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     **kwargs: Any,\n\u001b[32m    787\u001b[39m ) -> LLMResult:\n\u001b[32m    788\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1000\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    986\u001b[39m     run_managers = [\n\u001b[32m    987\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    988\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m    998\u001b[39m         )\n\u001b[32m    999\u001b[39m     ]\n\u001b[32m-> \u001b[39m\u001b[32m1000\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m   1008\u001b[39m     run_managers = [\n\u001b[32m   1009\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m   1010\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1017\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m   1018\u001b[39m     ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py:815\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    805\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    806\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    811\u001b[39m     **kwargs: Any,\n\u001b[32m    812\u001b[39m ) -> LLMResult:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    814\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    819\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    823\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    824\u001b[39m         )\n\u001b[32m    825\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    826\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/langchain_huggingface/llms/huggingface_pipeline.py:323\u001b[39m, in \u001b[36mHuggingFacePipeline._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m batch_prompts = prompts[i : i + \u001b[38;5;28mself\u001b[39m.batch_size]\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m responses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:321\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    320\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1439\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1436\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1437\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1438\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1439\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1365\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1364\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1366\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:419\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    417\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    422\u001b[39m     generated_sequence = output.sequences\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2617\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n\u001b[32m   2616\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2617\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2618\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2622\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2628\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2629\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2630\u001b[39m         input_ids=input_ids,\n\u001b[32m   2631\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2632\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2633\u001b[39m         **model_kwargs,\n\u001b[32m   2634\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3601\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3599\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3600\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3601\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3603\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3604\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3605\u001b[39m     outputs,\n\u001b[32m   3606\u001b[39m     model_kwargs,\n\u001b[32m   3607\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3608\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:959\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    958\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m959\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    961\u001b[39m     output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:474\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[32m    473\u001b[39m slice_indices = \u001b[38;5;28mslice\u001b[39m(-logits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/JHU/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "responses = []\n",
        "for reply in test_replies:\n",
        "    response = stance_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
        "    responses.append(response)\n",
        "\n",
        "# Print the results\n",
        "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
        "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljgw93Vr5grD"
      },
      "source": [
        "# Few-shot Prompt\n",
        "\n",
        "Now, Please create a LangChain FewShotPromptTemplate for classifying the stance of a reply to a comment. Use the following template for each example and create an example prompt using example_prompt for the examples in the few-shot prompt template:\n",
        "\n",
        "comment: [comment]\n",
        "reply: [reply]\n",
        "stance: [stance]\n",
        "\n",
        "Then, use the following structure for the few-shot prompt:\n",
        "prefix = '''Stance classification is the task of determining the expressed or implied opinion, or stance, of a reply toward a comment. The following replies express opinions about the associated comment. Each reply can either be \"agree\", \"disagree\", or \"neutral\" toward the comment.'''\n",
        "\n",
        "suffix = '''Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "stance:'''\n",
        "\n",
        "Create five few-shot examples with different comments and replies, including at least one for each possible stance: \"agree\", \"disagree\", and \"neutral\". Provide the code that constructs the FewShotPromptTemplate using the examples and the given prefix and suffix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mLuNW7HJ5grD"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import FewShotPromptTemplate\n",
        "\n",
        "# Define the prompt template for each example\n",
        "example_template = '''comment: {comment}\n",
        "reply: {reply}\n",
        "stance: {stance}'''\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"comment\", \"reply\", \"stance\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# Define the examples with various stances\n",
        "examples = [\n",
        "    {'comment': \"I think the new policy will help improve efficiency.\",\n",
        "     'reply': \"I agree, it will make things more streamlined.\",\n",
        "     'stance': 'agree'},\n",
        "\n",
        "    {'comment': \"The new education reform seems promising.\",\n",
        "     'reply': \"I disagree, it doesn't address the underlying issues.\",\n",
        "     'stance': 'disagree'},\n",
        "\n",
        "    {'comment': \"The park renovation project is a good idea.\",\n",
        "     'reply': \"I’m not sure. It may be good, but the location is an issue.\",\n",
        "     'stance': 'neutral'},\n",
        "\n",
        "    {'comment': \"Artificial intelligence will revolutionize healthcare.\",\n",
        "     'reply': \"I agree, it has the potential to save many lives.\",\n",
        "     'stance': 'agree'},\n",
        "\n",
        "    {'comment': \"The economy is showing signs of recovery after the pandemic.\",\n",
        "     'reply': \"I disagree, the recovery seems to be slow and uneven.\",\n",
        "     'stance': 'disagree'},\n",
        "]\n",
        "\n",
        "# Define the prefix and suffix for the few-shot prompt\n",
        "prefix = '''Stance classification is the task of determining the expressed or implied opinion, or stance, of a reply toward a comment. The following replies express opinions about the associated comment. Each reply can either be \"agree\", \"disagree\", or \"neutral\" toward the comment.'''\n",
        "\n",
        "suffix = '''Analyze the following reply to the provided comment and determine its stance. Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "stance:'''\n",
        "\n",
        "example_prompt.format(comment=\"I think the new policy will help improve efficiency.\",\n",
        "                       reply=\"I agree, it will make things more streamlined.\",\n",
        "                       stance=\"agree\")\n",
        "# Create the FewShotPromptTemplate using the provided prefix, suffix, and examples\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"comment\", \"reply\"],\n",
        "    example_separator=\"\\n\"\n",
        ")\n",
        "\n",
        "# Now you can use this prompt in your LangChain pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Ap4iCn0H5grF",
        "outputId": "a766df8f-3b75-4b62-ec05-1658b3920571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reply 1: The newer ones fail to live up to the sophistry of the older movies from the 70's.\n",
            "Stance:  disagree\n",
            "\n",
            "Reply 2: Frank Herbert wrote a lot of books.\n",
            "Stance:  disagree\n",
            "\n",
            "Reply 3: I think the new Dune movie better captures the spirit, if not the content, of Frank Herbert's philosophy.\n",
            "Stance:  disagree\n",
            "<|assistant|>\n",
            "disagree\n",
            "\n",
            "Reply 4: The quick red fox jumped over the lazy brown dog.\n",
            "Stance:  neutral\n",
            "\n",
            "Reply 5: Yeah, this new movie is a real masterpiece, lol!!\n",
            "Stance:  disagree\n",
            "\n"
          ]
        }
      ],
      "source": [
        "few_shot_chain = few_shot_prompt | llm\n",
        "\n",
        "responses = []\n",
        "for reply in test_replies:\n",
        "    response = few_shot_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
        "    responses.append(response)\n",
        "\n",
        "# Print the results\n",
        "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
        "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIJymZ2o5grF"
      },
      "source": [
        "# Chain-of-Thought Prompt\n",
        "\n",
        "Please generate code that uses chain-of-thought (CoT) prompting to classify the stance of a reply to a comment. The process should consist of two stages:\n",
        "\n",
        "1. First Stage (Explanatory Step):\n",
        "- Generate an explanation for the stance (agree, disagree, neutral) of the reply toward the comment.\n",
        "- Use a prompt template for this step with the variables \"comment\" and \"reply\".\n",
        "- Output: \"stance_reason\".\n",
        "\n",
        "2. Second Stage (Final Classification Step):\n",
        "- Based on the explanation from the first stage (\"stance_reason\"), determine the final stance of the reply.\n",
        "- Use a second prompt template for this step with the variables \"comment\", \"reply\", and \"stance_reason\".\n",
        "- Output: The final stance as \"agree\", \"disagree\", or \"neutral\".\n",
        "\n",
        "Use the \"RunnablePassthrough\" to pass the \"comment\" and \"reply\" variables from the first chain to the second chain, and chain the steps together using the \"|\" operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TeyNdhgS5grF"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "cot_template_1 = '''Stance classification is the task of determining the expressed or implied opinion, or stance, of the reply towards the comment.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "explanation:'''\n",
        "\n",
        "cot_prompt_1 = PromptTemplate(\n",
        "    input_variables=[\"comment\",\"reply\"],\n",
        "    template=cot_template_1\n",
        ")\n",
        "\n",
        "cot_chain_1 = cot_prompt_1 | llm\n",
        "\n",
        "cot_template_2 ='''Therefore, based on your explanation, {stance_reason}, what is the final stance? Respond with a single word: \"agree\", \"disagree\", or \"neutral\". Only return the stance as a single word, and no other text.\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "stance:'''\n",
        "\n",
        "cot_prompt_2 = PromptTemplate(\n",
        "    input_variables=[\"comment\",\"reply\",\"stance_reason\"],\n",
        "    template=cot_template_2\n",
        ")\n",
        "\n",
        "cot_chain_2 = cot_prompt_2 | llm\n",
        "\n",
        "cot_chain = {\"stance_reason\": cot_chain_1, \"comment\":RunnablePassthrough(), \"reply\":RunnablePassthrough()} | cot_chain_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srwt_GQN5grF",
        "outputId": "c0c7765f-44a2-4af5-a7c4-6967775c8500"
      },
      "outputs": [],
      "source": [
        "responses = []\n",
        "for reply in test_replies:\n",
        "    response = cot_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
        "    responses.append(response)\n",
        "\n",
        "# Print the results\n",
        "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
        "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7LDGvKW5grG"
      },
      "source": [
        "# Tree-of-Thought Prompt\n",
        "\n",
        "Please generate code that uses Tree-of-Thought (ToT) prompting to explore multiple reasoning paths and iteratively evaluate potential stances (agree, disagree, neutral) before making a final classification. Maintain the context of the comment and reply through all stages of reasoning and evaluation.\n",
        "\n",
        "Steps:\n",
        "Step 1 - Generate Hypotheses:\n",
        "- Propose multiple possible stances (agree, disagree, neutral) based on different interpretations of the reply.\n",
        "- Use a prompt template to generate each hypothesis with explanations.\n",
        "- Output: hypotheses as a list.\n",
        "\n",
        "Step 2 - Evaluate Hypotheses:\n",
        "- Assess the validity of each hypothesis by critically analyzing its reasoning.\n",
        "- Use an evaluation prompt template to rank or score each hypothesis based on coherence and relevance. Assign a score (1–5) for logical consistency and coherence\n",
        "- Output: evaluations as scores or rankings for each hypothesis.\n",
        "\n",
        "Step 3 - Final Decision:\n",
        "- Select the hypothesis with the highest score or best reasoning and output the final stance as \"agree\", \"disagree\", or \"neutral\" based on the reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pFnIKEc45grG"
      },
      "outputs": [],
      "source": [
        "# Step 1: Generate Hypotheses\n",
        "hypothesis_template = '''\n",
        "Consider the following comment and reply:\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "\n",
        "Generate three different hypotheses for the stance of the reply towards the comment.\n",
        "For each hypothesis, explain why the reply might:\n",
        "1. Agree\n",
        "2. Disagree\n",
        "3. Be Neutral\n",
        "\n",
        "Output each hypothesis clearly labeled (e.g., \"Hypothesis 1: ...\") with a newline between each hypothesis.'''\n",
        "\n",
        "hypothesis_prompt = PromptTemplate(\n",
        "    input_variables=[\"comment\", \"reply\"],\n",
        "    template=hypothesis_template\n",
        ")\n",
        "\n",
        "hypothesis_chain = hypothesis_prompt | llm\n",
        "\n",
        "# Step 2: Evaluate Hypotheses\n",
        "evaluation_template = '''\n",
        "Consider the following comment and reply:\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "\n",
        "Given the following hypotheses and explanations for the stance of the reply towards the comment:\n",
        "{hypotheses}\n",
        "\n",
        "Evaluate each hypothesis based on its logical consistency and support from the reply.\n",
        "Assign a numerical score from 1 to 5 for each hypothesis, where 5 is highly consistent and 1 is inconsistent. Only reply with the score and reason for that score for each hypothesis (e.g., Hypothesis 1: [score], reason: ...) and no other text.'''\n",
        "\n",
        "evaluation_prompt = PromptTemplate(\n",
        "    input_variables=[\"hypotheses\", \"comment\", \"reply\"],\n",
        "    template=evaluation_template\n",
        ")\n",
        "\n",
        "evaluation_chain = evaluation_prompt | llm\n",
        "\n",
        "# Step 3: Final Decision\n",
        "decision_template = '''\n",
        "Consider the following comment and reply:\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "\n",
        "Based on the evaluations of different hypotheses for the stance of the reply towards the comment:\n",
        "{hypotheses}\n",
        "\n",
        "{evaluations}\n",
        "\n",
        "Select the hypothesis with the highest score. Output the final stance as \"agree\", \"disagree\", or \"neutral\" based on the chosen hypothesis. Only output the label as single word and do not generate any other text after the label.\n",
        "label:'''\n",
        "\n",
        "decision_prompt = PromptTemplate(\n",
        "    input_variables=[\"hypotheses\", \"evaluations\", \"comment\", \"reply\"],\n",
        "    template=decision_template\n",
        ")\n",
        "\n",
        "decision_chain = decision_prompt | llm\n",
        "\n",
        "# Combine the chains into a Tree-of-Thought pipeline\n",
        "tot_chain = {\n",
        "    \"hypotheses\": hypothesis_chain,\n",
        "    \"comment\": RunnablePassthrough(),\n",
        "    \"reply\": RunnablePassthrough()\n",
        "} | RunnablePassthrough.assign(evaluations = evaluation_chain) | decision_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7SpwAAs5grG",
        "outputId": "74902665-b3c8-4fe3-e439-d30098dfac0b"
      },
      "outputs": [],
      "source": [
        "responses = []\n",
        "for reply in test_replies:\n",
        "    response = tot_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
        "    responses.append(response)\n",
        "\n",
        "# Print results\n",
        "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
        "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wfT2bft5grG"
      },
      "source": [
        "# Self-Consistency Prompt\n",
        "\n",
        "Given the final stance labels from multiple reasoning approaches—Tree-of-Thought (ToT), Chain-of-Thought (CoT), Few-Shot prompting, and Task-only approach—determine the final stance label (\"agree\", \"disagree\", or \"neutral\") by synthesizing the outputs.\n",
        "\n",
        "Inputs:\n",
        "- Comment and Reply: The context for determining the stance.\n",
        "- Stance Labels from the four approaches:\n",
        "    - tot_output: Stance label from the Tree-of-Thought (ToT) approach.\n",
        "    - cot_output: Stance label from the Chain-of-Thought (CoT) approach.\n",
        "    - few_shot_output: Stance label from the Few-Shot prompting approach.\n",
        "    - task_output: Stance label from the Task-only approach.\n",
        "\n",
        "Steps:\n",
        "1. Compare the stance labels from the four approaches.\n",
        "2. Identify patterns of agreement or disagreement:\n",
        "- If there is a majority consensus, select that stance label.\n",
        "- If there is no clear majority, resolve the inconsistencies by choosing the most consistent or compelling label based on the distribution of outputs.\n",
        "3. Output: A final stance label of \"agree\", \"disagree\", or \"neutral\", based on the most consistent or majority label. Only output the label, with no additional explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6zgw_ha5grG"
      },
      "outputs": [],
      "source": [
        "# Self-Consistency Prompt Template\n",
        "consistency_template = '''\n",
        "Consider the following comment and reply:\n",
        "comment: {comment}\n",
        "reply: {reply}\n",
        "\n",
        "You have been provided with stance outputs generated by four different approaches:\n",
        "1. Tree-of-Thought (ToT) approach: {tot_output}\n",
        "2. Chain-of-Thought (CoT) approach: {cot_output}\n",
        "3. Few-Shot approach: {few_shot_output}\n",
        "4. Task-only approach: {task_output}\n",
        "\n",
        "Compare these outputs and determine the most likely stance label. Output the final stance label as \"agree\", \"disagree\", or \"neutral\" based on the most consistency across the responses. Only output the label as single word and do not generate any other text after the label.\n",
        "'''\n",
        "\n",
        "consistency_prompt = PromptTemplate(\n",
        "    input_variables=[\"comment\", \"reply\", \"tot_output\", \"cot_output\", \"few_shot_output\", \"task_output\"],\n",
        "    template=consistency_template\n",
        ")\n",
        "\n",
        "self_evaluation_chain = consistency_prompt | llm\n",
        "\n",
        "# Combine chains into a pipeline with pass-through variables\n",
        "consistency_chain = {\n",
        "    \"tot_output\": tot_chain,\n",
        "    \"cot_output\": cot_chain,\n",
        "    \"few_shot_output\": few_shot_chain,\n",
        "    \"task_output\": stance_chain,\n",
        "    \"comment\": RunnablePassthrough(),\n",
        "    \"reply\": RunnablePassthrough()\n",
        "} | self_evaluation_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkwS4s5-5grG",
        "outputId": "11f4197e-c5b3-4300-ced7-6ffc737a7068"
      },
      "outputs": [],
      "source": [
        "responses = []\n",
        "for reply in test_replies:\n",
        "    response = consistency_chain.invoke({\"comment\": test_comment, \"reply\": reply})\n",
        "    responses.append(response)\n",
        "\n",
        "# Print results\n",
        "for idx, (reply, response) in enumerate(zip(test_replies, responses)):\n",
        "    print(f\"Reply {idx+1}: {reply}\\nStance: {response}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCu2A_wD5grH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
