{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the spacy package if you have not already done so.  Once you have installed spacy, you no longer need to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the pandas and spacy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "Create some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Twitter data\n",
    "data = {\n",
    "    \"tweet\": [\n",
    "        \"I love the new iPhone! It's fantastic. #Apple\",\n",
    "        \"The service at this restaurant was terrible. Never going back. #Disappointed\",\n",
    "        \"Tesla's new model is groundbreaking! #Innovation\",\n",
    "        \"I had an average experience with the product. It's okay. #Neutral\",\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "The first technical step in preprocessing is tokenization, which breaks down text into smaller units called tokens.  Tokens are the building blocks for all subsequent preprocessing steps. It enables models to process text effectively by isolating meaningful components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization with spaCy\n",
    "df[\"tokens\"] = df[\"tweet\"].apply(lambda x: [token.text for token in nlp(x)])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words\n",
    "Next, we clean the text by removing stop words.  Stop words are common words like “the,” “is,” “and,” which occur frequently but provide little semantic value.  Removing them reduces noise and allows the model to focus on meaningful terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop]  # Exclude stop words\n",
    "    return tokens\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df[\"tokens\"] = df[\"tweet\"].apply(remove_stopwords)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "After cleaning, we standardize text using stemming or lemmatization. Stemming reduces words to their root form by removing suffixes and prefixes. Lemmatization converts words to their base or dictionary form using linguistic rules.  Lemmitization is more accurate than stemming but computationally heavier. Note the clock difference between running the stop word code block and the lemmitization code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform lemmatization on stopword-removed tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    doc = nlp(\" \".join(tokens))  # Recreate a string from tokens\n",
    "    lemmas = [token.lemma_ for token in doc]  # Lemmatize the tokens\n",
    "    return lemmas\n",
    "\n",
    "# Apply stop word removal\n",
    "df[\"tokens_no_stop\"] = df[\"tweet\"].apply(remove_stopwords)\n",
    "\n",
    "# Apply lemmatization on tokens without stop words\n",
    "df[\"lemmas\"] = df[\"tokens_no_stop\"].apply(lemmatize_tokens)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Visualize the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for visualization\n",
    "df_visualization = pd.DataFrame({\n",
    "    \"Original Tweet\": df[\"tweet\"],\n",
    "    \"Tokens\": df[\"tokens_no_stop\"].apply(lambda x: \", \".join(x)),  # Display tokens without stop words\n",
    "    \"Lemmatized Tokens\": df[\"lemmas\"].apply(lambda x: \", \".join(x))  # Display lemmatized tokens\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_visualization)\n",
    "\n",
    "# Optional: Render the DataFrame as an HTML table for better visualization (e.g., in a Jupyter Notebook)\n",
    "from IPython.display import display\n",
    "display(df_visualization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
